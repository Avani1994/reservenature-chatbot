{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mistralai.client import MistralClient\n",
    "from mistralai.models.chat_completion import ChatMessage\n",
    "import numpy as np\n",
    "import os\n",
    "import faiss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You API Key and intitialize mistral client\n",
    "api_key = \"YOUR API KEY\"\n",
    "#model = \"mistral-large-latest\"\n",
    "\n",
    "client = MistralClient(api_key=api_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "33\n"
     ]
    }
   ],
   "source": [
    "# split the datset into chunks\n",
    "# smaller length chunks do help while in longer length chunks information gets lost\n",
    "# but we need maintain coherence between the chunks\n",
    "# splitting based on indidvidual questions answer should help as well but that might not generalize well\n",
    "# instead splitting based on headings sub headings, pargraph, but we have to keep in mind longer chunk sizes are not going to help \n",
    "# and too small might not capture the information\n",
    "# balance needs to bemaintained with experimentation\n",
    "f = open(\"dataset.txt\", \"r\")\n",
    "text = f.read()\n",
    "\n",
    "chunk_size = 512\n",
    "chunks = [text[i:i + chunk_size] for i in range(0, len(text), chunk_size)]\n",
    "print(len(chunks))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_text_embedding(input):\n",
    "    embeddings_batch_response = client.embeddings(\n",
    "          model=\"mistral-embed\",\n",
    "          input=input\n",
    "      )\n",
    "    return embeddings_batch_response.data[0].embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get mistral embeddings for similarity matching\n",
    "text_embeddings = np.array([get_text_embedding(chunk) for chunk in chunks])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a vector database for storing all the embeddings \n",
    "#we are using open source faise vector database, but others can be explored as well\n",
    "d = text_embeddings.shape[1]\n",
    "index = faiss.IndexFlatL2(d)\n",
    "index.add(text_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"I had just logged on and saw that my tracker was expired. I didn't realize it expired as I hadn't seen any info on your web site regarding that. Can I get it going again or do I need to make a new tracker?\"\n",
    "# get embeddings for question\n",
    "question_embeddings = np.array([get_text_embedding(question)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Seach for similar embeddings using nearest neighbors approach and distance as metric\n",
    "D, I = index.search(question_embeddings, k=2) # distance, index\n",
    "retrieved_chunk = [chunks[i] for i in I.tolist()[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = f\"\"\"\n",
    "Context information is below.\n",
    "---------------------\n",
    "{retrieved_chunk}\n",
    "---------------------\n",
    "Given the context information and not prior knowledge, answer the query.\n",
    "Query: {question}\n",
    "Answer:\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_mistral(user_message, model=\"mistral-medium-latest\"):\n",
    "    messages = [\n",
    "        ChatMessage(role=\"user\", content=user_message)\n",
    "    ]\n",
    "    chat_response = client.chat(\n",
    "        model=model,\n",
    "        messages=messages\n",
    "    )\n",
    "    return (chat_response.choices[0].message.content)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Based on the provided context information, it seems that you can't renew an expired tracker. Instead, you would need to create a new one. However, the context doesn't explicitly state that expired trackers can't be renewed, so it would be best to reach out to the support team to confirm. They may be able to help you restore the expired tracker or guide you through creating a new one.\n",
      "\n",
      "In summary, while it appears that you may need to create a new tracker, it is recommended to contact the support team for clarification and assistance.\n"
     ]
    }
   ],
   "source": [
    "print(run_mistral(prompt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "question2 = \"How to use Flexible dates feature for tracker\" \n",
    "question2_embeddings = np.array([get_text_embedding(question2)])\n",
    "D, I = index.search(question_embeddings, k=3) # distance, index\n",
    "retrieved_chunk = [chunks[i] for i in I.tolist()[0]]\n",
    "prompt2 = f\"\"\"\n",
    "Context information is below.\n",
    "---------------------\n",
    "{retrieved_chunk}\n",
    "---------------------\n",
    "Given the context information and not prior knowledge, answer the query.\n",
    "Query: {question2}\n",
    "Answer:\n",
    "\"\"\"\n",
    "# 3 most similar embeddings helped rather than 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To use the Flexible dates feature for a tracker, follow these steps:\n",
      "\n",
      "1. Begin creating a tracker as you normally would.\n",
      "2. When selecting dates, look for an option to check \"Flexible +-1\" or \"+-2\" days in small orange font.\n",
      "3. This option will search for availability within 1 or 2 days before or after your specified date(s).\n",
      "4. Make sure you have the Adventurer Mon subscription, as this feature is only available for that tier.\n",
      "5. Check the box for the desired flexible date range.\n",
      "6. Continue creating your tracker and save it. The tracker will now look for availability within the flexible date range you've chosen.\n"
     ]
    }
   ],
   "source": [
    "print(run_mistral(prompt2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Evaluation Technique\n",
    "#We can feed the response from LLM into it again and ask whether it \n",
    "#think it has answered question correctly and if yes then with how much percentage\n",
    "answer2 = f\"\"\"To use the Flexible dates feature for a tracker, follow these steps:\n",
    "\n",
    "1. Begin creating a tracker as you normally would.\n",
    "2. When selecting dates, look for an option to check \"Flexible +-1\" or \"+-2\" days in small orange font.\n",
    "3. This option will search for availability within 1 or 2 days before or after your specified date(s).\n",
    "4. Make sure you have the Adventurer Mon subscription, as this feature is only available for that tier.\n",
    "5. Check the box for the desired flexible date range.\n",
    "6. Continue creating your tracker and save it. The tracker will now look for availability within the flexible date range you've chosen.\"\"\"\n",
    "\n",
    "prompt_e = f\"\"\"\n",
    "Context information is below.\n",
    "---------------------\n",
    "{retrieved_chunk}\n",
    "---------------------\n",
    "Given the context information, do you think, you have answered your question correctly for following question and answer. \n",
    "Can you please answer yes or no with the percentage confidence you have in the given answer.\n",
    "Query: {question2}\n",
    "Answer: {answer2}\n",
    "\"\"\"\n",
    "# 3 most similar embeddings helped rather than 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Yes, 100%\n",
      "\n",
      "The given answer correctly describes the steps to use the Flexible dates feature for a tracker, as outlined in the context information. The answer accurately mentions the orange font, the availability search within 1 or 2 days before or after the specified date(s), and the requirement of the Adventurer Mon subscription for the feature.\n"
     ]
    }
   ],
   "source": [
    "print(run_mistral(prompt_e))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "question3 = \"Which number will I get call from when reservenature notifies about availability\" \n",
    "question3_embeddings = np.array([get_text_embedding(question3)])\n",
    "D, I = index.search(question3_embeddings, k=3) # distance, index\n",
    "retrieved_chunk = [chunks[i] for i in I.tolist()[0]]\n",
    "prompt3 = f\"\"\"\n",
    "Context information is below.\n",
    "---------------------\n",
    "{retrieved_chunk}\n",
    "---------------------\n",
    "Given the context information and not prior knowledge, answer the query. \n",
    "Please only answer the question if you have 100% confidence, other wise please say you don't know\n",
    "Query: {question3}\n",
    "Answer:\n",
    "\"\"\"\n",
    "#(wrong answer) --> You will get a call from +1 (669) 208-5607 when Reserve Nature notifies you about availability, with 95% confidence interval\n",
    "#for this question may be adding this question in multiple forms in dataset.txt will help. \n",
    "#Seems like we this information is lost in longer chunk size. May be using smaller chunk size but then it will affect other questions \n",
    "#whose information is captured in longer chunk size\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The contact number for Reserve Nature is +1 (669) 208-5607. Therefore, you will likely receive a call from this number when Reserve Nature notifies you about availability. However, I cannot say this with 100% certainty as the text does not explicitly state that this is the number they will use to call customers about availability. It is possible that they may use a different number for this purpose. If you want to confirm, you may want to contact Reserve Nature directly.\n"
     ]
    }
   ],
   "source": [
    "print(run_mistral(prompt3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Evaluation Technique (may be try a different LLM), and try to use the answer whichis average answer, if answers embeddings are close?\n",
    "#We can feed the response from LLM into it again and ask whether it \n",
    "#think it has answered question correctly and if yes then with how much percentage\n",
    "answer3 = f\"\"\"\n",
    "You will get a call from +1 (669) 208-5607 when Reserve Nature notifies you about availability.\n",
    "\"\"\"\n",
    "\n",
    "prompt_e = f\"\"\"\n",
    "Context information is below.\n",
    "---------------------\n",
    "{retrieved_chunk}\n",
    "---------------------\n",
    "Given the context information, do you think, you have answered your question correctly for following question and answer. \n",
    "Can you please answer yes or no with the percentage confidence you have in the given answer.\n",
    "Query: {question3}\n",
    "Answer: {answer3}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Yes, 99% confident. The phone number +1 (669) 208-5607 is listed in the provided context information as a contact number for Reserve Nature.\n"
     ]
    }
   ],
   "source": [
    "print(run_mistral(prompt_e))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "question3 = \"Will I get refund if I don't find availability for my campsite?\" \n",
    "question3_embeddings = np.array([get_text_embedding(question3)])\n",
    "D, I = index.search(question3_embeddings, k=3) # distance, index\n",
    "retrieved_chunk = [chunks[i] for i in I.tolist()[0]]\n",
    "prompt3 = f\"\"\"\n",
    "Context information is below.\n",
    "---------------------\n",
    "{retrieved_chunk}\n",
    "---------------------\n",
    "Given the context information and not prior knowledge, answer the query. \n",
    "Please only answer the question if you have 100% confidence, other wise please say you don't know\n",
    "Query: {question3}\n",
    "Answer:\n",
    "\"\"\"\n",
    "# point to be noted\n",
    "# docs need to be updated if there's any change. offer was for limited time, but it's still on docs, so llm just uses that info\n",
    "# update docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Yes, according to the context information, if the service is unable to find a spot for your desired campsite, they will give 100% credits back (limited time offer).\n"
     ]
    }
   ],
   "source": [
    "print(run_mistral(prompt3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "question3 = \"How to Cancel Subscription / Pause Subscription\" \n",
    "question3_embeddings = np.array([get_text_embedding(question3)])\n",
    "D, I = index.search(question3_embeddings, k=3) # distance, index\n",
    "retrieved_chunk = [chunks[i] for i in I.tolist()[0]]\n",
    "prompt3 = f\"\"\"\n",
    "Context information is below.\n",
    "---------------------\n",
    "{retrieved_chunk}\n",
    "---------------------\n",
    "Given the context information and not prior knowledge, answer the query. \n",
    "Please only answer the question if you have 100% confidence, other wise please say you don't know\n",
    "Query: {question3}\n",
    "Answer:\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To cancel your subscription, you can click on the top right menu and select \"Dashboard\". Then, select Subscriptions from the grid. You will see options to cancel your subscription. To pause your subscription, follow the same steps until you reach the Subscriptions page. There, you will see an option to pause your subscription instead of canceling it. Note that when you pause your subscription, you still have the subscription until the next billing date. After that, your account won't be debited for the duration of the pause, and you are free to resume the subscription anytime you want.\n"
     ]
    }
   ],
   "source": [
    "print(run_mistral(prompt3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "reservenature-chatbot",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
